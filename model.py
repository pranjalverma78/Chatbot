# -*- coding:  utf-8 -*-
"""Database2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuotYtqxFGhzQV3ZYYwQeD8cVLprUT-w
"""
import torch
# import chromadb
import logging
import os
import re
import requests
import faiss
import json
import sentence_transformers
# from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
# from googletrans import Translator


# Now the dataset variable contains the text from the file
# print(datast)

# translator = Translator()-------------------------------------------
# from langchain_text_splitters import CharacterTextSplitter

# from google.colab import drive
# drive.mount('/content/drive')
# folder_path = "drive/MyDrive/books"

folder_path = "/books"
access_token="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"

f_path_ins = 'final_inst.txt'  # Adjust the path if the file is in a different directory
f_dataset = 'dataset.txt'
# Read the content of the file and store it in a variable
with open(f_path_ins, 'r') as file:
    instruction = file.read()

with open(f_dataset, 'r') as file:
    dataset = file.read()

# !pip install chromadb -q
# !pip install sentence-transformers -q

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    print("GPU is available")
else:
    print("GPU is not available")

index = faiss.read_index('faiss_index.bin')
with open('document_metadata.json', 'r') as f:
    file_data = json.load(f)


index2 = faiss.read_index('faiss_query.bin')
with open('query_metadata.json', 'r') as f:
    documents = json.load(f)


embedding_function = SentenceTransformer('all-MiniLM-L6-v2').to(device)

formatted_results = []
tt = ""

def search_content2(query,k=3):

    query_vector = embedding_function.encode([query])

    faiss.normalize_L2(query_vector)

    distances, indices = index2.search(query_vector, k)
    results = []
    for idx, dist in zip(indices[0], distances[0]):
        result = documents[idx]
        # result['similarity'] = 1 - dist
        results.append(result)

    result = '\n\n'.join(item for item in results)

    return result

def search_content(query,k=3):
    # Create an embedding for the query
    
    query_vector = embedding_function.encode([query])
    
    # Normalize the query vector
    faiss.normalize_L2(query_vector)
    
    # Search the FAISS index
    distances, indices = index.search(query_vector, k)
    
    # Collect results
    results = []
    for idx, dist in zip(indices[0], distances[0]):
        result = file_data[idx].copy()
        result['similarity'] = 1 - dist  # Convert distance to similarity
        results.append(result)

    result = '\n\n'.join(item['page_content'] for item in results)

    # for i, item in enumerate(results, 1):
    #     formatted_results.append(f"Result {i} - Similarity:  {item['similarity']:.4f}\n\n{item['page_content']}")

    # tt = '\n\n'.join(formatted_results)

    return result

# Example usage
# query = "what is peace?"
# result = search_content(query)
# print(result)


"""Using Llama Models"""

s = requests.Session()
api_base = "https://api.endpoints.anyscale.com/v1"
# Replace with long-lived credentials for production
token = "esecret_9e8ndpqewktu8apjfzzj65fn3a"
url = f"{api_base}/chat/completions"


def generate_txt(query,max_length=400):
    query = query.lower()
    torch.cuda.empty_cache()
    input_text = search_content(query)
    input_text = input_text + "Instructions must be followed"
    # input_text2 = search_content(query)
    # input_text2 = input_text2 + "Instructions must be followed"
    # print(input_text)
    # messages = [
    #     {"role":  "user", "content":  input_text},
    # ]

    # print(type(dataset))

    full_input = f"Instruction:{instruction}\n\nQuery:  {query}\n\nContext:  {input_text}\n\nDataset: {dataset}"
    
    full_input = f'''<s> [INST] <<SYS>>
    {instruction}
    ____
    I am about to feed you some data. This contains the question answer pairs on how you are supposed to respond to a query. Here you go: 
    {dataset}
    <</SYS>>

    Following are some retrieved context that might be relevant for the query:
    {input_text}
    Do not ask question from user.
    Follow these additional instructions strictly Chat in a very friendly way like best friend and human way with the user make response concise and precise. 
    As well be best sellsmen of Sankalpam services depending upon query. Do not ask questions strictly.
    Now, I give you the query:
    {query}
    [/INST]
    '''
    # print(full_input)


    body = {
      "model":  "meta-llama/Meta-Llama-3-70B-Instruct",
      "messages":  [
          {"role":  "user", "content":  full_input}
      ],
      "temperature":  0.2,
      "max_tokens":  400,
      "top_p":  1,
      "frequency_penalty":  0.95
    }


    with s.post(url, headers={"Authorization":  f"Bearer {token}"}, json=body) as resp:
      response =  resp.json()['choices'][0]['message']['content']

    # translated_text = translator.translate(response, dest='hi')
    # return translated_text.text
    return response

input_text="why?"

response = generate_txt(input_text)
print(response)
# matching = search_content(input_text)
# print(matching)

# text